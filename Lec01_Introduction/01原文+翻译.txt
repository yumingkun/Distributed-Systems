6.824 2016 Lecture 1: Introduction

6.824: Distributed Systems Engineering

什么是分布式系统
What is a distributed system?
  多个合作的计算机
  multiple cooperating computers
  绝大部分关键的基础设施都是分布式的
  DNS, P2P file sharing, big databases, MapReduce, &c
  lots of critical infrastructure is distributed!

为什么要用分布式
Why distribute?
  抽象多个物理实体为一个虚拟实体
  to connect physically separate entities
  通过隔离实现安全，毕竟一个大型机挂了就是挂了，集群挂了一个，把他屏蔽了，使用其他的
  to achieve security via isolation
  通过复制？容忍错误
  to tolerate faults via replication
  通过增加相同的主机弹性增强吞吐量
  to scale up throughput via parallel CPUs/mem/disk/net

但是！
But:
  复杂：很多部分需要大量并发
  complex: many concurrent parts
  要能处理错误情况
  must cope with partial failure
  代码必须有高性能
  tricky to realize performance potential

Why take this course?
  interesting -- hard problems, non-obvious solutions
  used by real systems -- driven by the rise of big Web sites
  active research area -- lots of progress + big unsolved problems
  hands-on -- you'll build serious systems in the labs

COURSE STRUCTURE

http://pdos.csail.mit.edu/6.824

Course staff:
  Robert Morris, lecturer
  Frans Kaashoek, lecturer
  Steven Allen, TA
  Stephanie Wang, TA
  Jon Gjengset, TA
  Daniel Ziegler, TA

Course components:
  lectures
  readings
  two exams
  labs
  final project

读论文，做实验，上课什么的听力渣渣就别想了
Lectures about big ideas, papers, and labs

研究论文作为案例
Readings: research papers as case studies
  上课前腰读论文
  please read papers before class
    otherwise boring, and you can't pick it up by listening
  每个论文都有问题，你需要回答一下
  each paper has a short question for you to answer
  and you must send us a question you have about the paper
  submit question&answer by 10pm the night before

Mid-term exam in class, and final exam

实验的目标
Lab goals:
  深入理解一些技术
  deeper understanding of some important techniques
  有分布式变成经验
  experience with distributed programming
  first lab is due a week from Friday

第一个实验，MapReduce
Lab 1: MapReduce
错误恢复
Lab 2: replication for fault-tolerance
错误存储
Lab 3: fault-tolerant key/value store
共享存储
Lab 4: sharded key/value store

Final project at the end, in groups of 2 or 3.
  You can think of a project and clear it with us.
  Or you can do a "default" project that we'll specify.

Lab grades depend on how many test cases you pass
  we give you the tests, so you know whether you'll do well
  careful: if it usually passes, but sometimes fails,
    chances are it will fail when we run it

Lab code review
  look at someone else's lab solution, send feedback
  perhaps learn about another approach

Debugging the labs can be time-consuming
  start early
  come to TA office hours
  ask questions on Piazza

MAIN TOPICS

这个课是关于被高层应用使用的基础架构
This is a course about infrastructure, to be used by applications.
     为高层应用隐藏底层架构细节 
  About abstractions that hide complexity of distribution from applications.
  这里有三大抽象
  Three big kinds of abstraction:
  	存储、通讯、计算
    Storage.
    Communication.
    Computation.
  下面还有一些主题会被反复提起
  A couple of topics come up repeatedly.

RPC、线程、并发控制的实现
Topic: implementation
  RPC, threads, concurrency control.

性能
Topic: performance
	理想：可伸缩的吞吐量
  The dream: scalable throughput.
  	可以通过添加并行的硬件提升总的吞吐量，理论上处理更多的负载只需要添加机器
    Nx servers -> Nx total throughput via parallel CPU, disk, net.
    So handling more load only requires buying more computers.
  伸缩性会变的越来越难
  Scaling gets progressively harder:
  	负载均衡，单机性能故障(或反映太慢)
    Load balance, stragglers.
          不可并行的部分
    "Small" non-parallelizable parts.
          隐藏被共享的资源
    Hidden shared resources, e.g. network.

错误处理
Topic: fault tolerance
  林子大了啥鸟都有，机器多了总有几个不听话的
  1000s of server, complex net -> always something broken
  不应该让上层应用感知到有机器出错了
  We'd like to hide these failures from the application.
	想要高可用、持久性  
  We often want:
    Availability -- I can keep using my files despite failures
    Durability -- my data will come back to life when failures are repaired
    总结起来就是错误恢复
  Big idea: replicated servers.
    If one server crashes, client can proceed using the other.

一致性
Topic: consistency
	目的：所有基础设施应该保持一致性
  General-purpose infrastructure needs well-defined behavior.
  Get(k)获取最近放入的Put(k,v)，注意这个Put操作可能在A主机上实现，而Get在B主机上运行，他们之间数据应该保持一致
    E.g. "Get(k) yields the value from the most recent Put(k,v)."
  达到一致性有点难
  Achieving good behavior is hard!
  	客户端并发提交，某个服务器突然尴尬的宕机了，Network可能让核心节点以为某个反映太慢的节点死亡了
    Clients submit concurrent operations.
    Servers crash at awkward moments.
    Network may make live servers look dead; risk of "split brain".
     一致和性能不可兼得
  Consistency and performance are enemies.
  	一致需要通讯，严格实现一致的服务太慢了，快的系统不想要应用处理复杂行为
    Consistency requires communication, e.g. to get latest Put().
    Systems with pleasing ("strict") semantics are often slow.
    Fast systems often make applications cope with complex ("relaxed") behavior.
    大佬们已经在这些问题上做了一些列的探究
  People have pursued many design points in this spectrum.


案例研究
CASE STUDY: MapReduce

Let's talk about MapReduce (MR) as a case study
  MR is a good illustration of 6.824's main topics
  and is the focus of Lab 1

总览一下
MapReduce overview
	几小时实现几T数据分布计算
  context: multi-hour computations on multi-terabyte data-sets
  	爬取web页面结构的实验分析
    e.g. experimental analyses of structure of crawled web pages
	如果你不喜欢分布式，开发会很痛苦的
    often not developed by distributed systems enthusiasts
    can be very painful, e.g. coping with failure
  整体目标：非专业人士的程序也能很简单的用合理的效率解决解决大数据问题，
  overall goal: non-specialist programmers can easily solve giant
    data processing problems with reasonable efficiency.
  程序定义了Map和Reduce函数，有序的代码总是清晰的
  programmer defines Map and Reduce functions
    sequential code; often fairly simple
  MR有着巨大的数据量，运行在1000个机器以上，并且隐藏了所有实现细节
  MR runs the functions on 1000s of machines with huge inputs
    and hides all details of distribution
  
MR的总览视角
Abstract view of MapReduce
  输入被分割到 "splits"
  input is divided into "splits"

  先对要处理的数据做分割，分别传入到多个map函数中，传入(k1,v1)
  map函数对接受的数据做处理，生成(k2,v2)，再交给reduce汇总
  reduce从不同的map中接收到(k2,v2)，汇总为(k2,v3)
  Input Map -> a,1 b,1 c,1
  Input Map ->     b,1
  Input Map -> a,1     c,1
                |   |   |
                    |   -> Reduce -> c,2
                    -----> Reduce -> b,2
  MR calls Map() on each split, produces set of k2,v2
    "intermediate" data
  MR gathers all intermediate v2's for a given k2,
    and passes them to a Reduce call
  final output is set of <k2,v3> pairs from Reduce()

例如：计算上千个文件中单词出现的个数
    对每个文件调用map函数，传入文件名和文件内容(应该再传入一个用户可以自定义的处理函数)
    map遍历文件，并统计每个单词的数量，生成单个文件的 []map[word]times 数组
    把这个数组交给reduct汇总，得到所有文件的 []map[word]times
Example: word count
  input is thousands of text files
  Map(k, v)
    split v into words
    for each word w
      emit(w, "1")
  Reduce(k, v)
    emit(len(v))

这个模型很容易实现，它隐藏了很多应该处理的错误
This model is easy to program; it hides many painful details:
  并发 -- 顺序执行的结果
  concurrency -- same result as sequential execution
  starting s/w on servers
  data movement
  failures

这个模型的伸缩性很好
This model scales well.
  多机器间不用相互依赖等待，他们并行运行
  Nx computers get you Nx Map() and Reduce() throughput.
    Map()s don't wait for each other or share data, csan run in parallel.
    Same for Reduce()s.
    Up to a point...
  因此你能通过增加机器增加吞吐量
  So you can get more throughput by buying more computers.
    Rather than special-purpose efficient parallelizations of each application.
    Computers are much cheaper than programmers!

性能瓶颈在哪
What will be the limiting factor in performance?
  我们很关心瓶颈，这是需要优化的地方
  We care since that's the thing to optimize.
  CPU? memory? disk? network?
  被带宽限制了，上千个机器一起运算很快，但是很少有机器的带宽能承受上千机器传输，所以尽可能压缩传输数据
  They were limited by "network cross-section bandwidth".
    [diagram: map servers, network box, reduce servers]
    The network's total internal capacity.
    Often much smaller than sum of host network link speeds.
  Hard to build a network than can run 1000x faster than a single computer.
  So they cared about minimizing movement of data over the network.

容错处理呢
What about fault tolerance?
  如果有一个MR节点宕机？
  该程序很大一部分就是用来容错的
  肯定不能因为某个节点错误将所有节点重启，重新计算，只是将错误节点重启
  I.e. what if a server crashes during a MR job?
  Hiding failures is a huge part of ease of programming!
  Why not re-start the whole job from the beginning?
  MR re-runs just the failed Map()s and Reduce()s.
    每个节点是纯功能型的，不修改任何输入，不维持状态，不共享内存，不和其他的map联系，即，节点间相互不依赖
    They are pure functions -- they don't modify their inputs,
      they don't keep state, there's no shared memory, there's
      no map/map or reduce/reduce interaction.
    所以重新执行可以产生相同的结果
    So re-execution is likely to yield the same output.
  没有副作用的纯功能型函数是这个模型的主要限制
  The requirement for pure functions is a major limitation of
    MR compared to other parallel programming schemes.
    But it's critical to MR's simplicity.


更多细节
More details (paper's Figure 1):
  master:分配任务给workers，记忆中间输出的位置
  master: gives tasks to workers; remembers where intermediate output is
  M input splits
  输入存储在GFS中，所有机器运行在GFS上，输入的splits比works更多
  input stored in GFS, 3 copies of each split
  all computers run both GFS and MR workers
  many more input splits than workers
  master在每一个serever上开启任务，旧任务结束后开启新任务
  master starts a Map task on each server
    hands out new tasks as old ones finish
  worker按照key将输出保存在磁盘中
  worker hashes Map output by key into R partitions, on local disk
  直到所有的map调用结束后，开始reduce调用
  no Reduce calls until all Maps are finished
  master指挥reducer从map worker中获取中间数据，并写入最终数据到GFS中
  master tells Reducers to fetch intermediate data partitions from Map workers
  Reduce workers write final output to GFS

怎样能增强网络性能
    Map的输出从disk中读取
    中间数据只通过网络传输一次，并存储在disk中，而不是GFS
    中间数据按照key被划分多个文件，大的文件传输更高效
How does detailed design help network performance?
  Map input is read from local disks, not over network.
  Intermediate data goes over network just once.
    Stored on local disk, not GFS.
  Intermediate data partitioned into files holding many keys.
    Big network transfers are more efficient.

怎样实现好的负载均衡
    在不同大小、内容、硬件加运行某需要的时间是不一样的
    解决方案：创造比workers更多的splits
        不断的给完成任务的worker分配新的任务
        所以split不能很大，应该控制着运行的时间
        这样，性能强的机器处理任务更多，慢的少一点，但尽可能同时处理完
How do they get good load balance?
  Critical to scaling -- otherwise Nx servers -> no gain.
  But time to process a split or partition isn't uniform.
    Different sizes and contents, and different server hardware.
  Solution: many more splits than workers.
    Master hands out new splits to workers who finish previous tasks.
    So no split is so big it dominates completion time (hopefully).
    So faster servers do more work than slower ones, finish abt the same time.

MR如何处理worker宕机
    worker宕机：
        重启master，通过GFS发布任务,即使一些worker已经完成任务了，还是需要他们硬盘上的数据
        一些reduct的work可能会读取中间数据失败，所以我们需要函数式的确定的Map()
    master节点怎么知道哪个节点宕机呢,Pings

How does MR cope with worker crashes?
  * Map worker crashes:
    master re-runs, spreads tasks over other GFS replicas of input.
      even if worker had finished, since still need intermediate data on disk.
    some Reduce workers may already have read failed worker's intermediate data.
      here we depend on functional and deterministic Map()!
    how does the master know the worker crashed? (pings)
    master need not re-run Map if Reduces have fetched all intermediate data
      though then a Reduce crash would have to wait for Maps to re-run
  * Reduce worker crashes before producing output.
    master re-starts its tasks on another worker.
  * Reduce worker crashes in the middle of writing its output.
    GFS has atomic rename that prevents output from being visible until complete.
    so it's safe for the master to re-run the Reduce tasks somewhere else.

其他一些问题
    如果master不小心mao()两次，它应该告诉reduce只处理一次
    如果对同一部分做了两次Reduce()，他们应该写入相同的输出在GFS上，GFS的原子操作将被出发，只有一个能成功写入
    如果一个节点可能因为垃圾硬件工作太慢，master应该对最后的这些任务做二次拷贝
    如果一个worker因为软件或硬件计算错误结果怎么办，没办法了。。。
    如果master崩了咋办
Other failures/problems:
  * What if the master accidentally starts *two* Map() workers on same input?
    it will tell Reduce workers about only one of them.
  * What if two Reduce() workers for the same partition of intermediate data?
    they will both try to write the same output file on GFS!
    atomic GFS rename will cause the second to finish to win.
  * What if a single worker is very slow -- a "straggler"?
    perhaps due to flakey hardware.
    master starts a second copy of last few tasks.
  * What if a worker computes incorrect output, due to broken h/w or s/w?
    too bad! MR assumes "fail-stop" CPUs and software.
  * What if the master crashes?

哪些应用不适合在MR上运行
    不是任何都适合这种模型
    小数据不适合
    大数据的小更新不适合，还是小数据。。。
    不稳定的读取不适合
    可以实现更灵活的系统，但是带来更复杂的模型
For what applications *doesn't* MapReduce work well?
  Not everything fits the map/shuffle/reduce pattern.
  Small data, since overheads are high. E.g. not web site back-end.
  Small updates to big data, e.g. add a few documents to a big index
  Unpredictable reads (neither Map nor Reduce can choose input)
  Multiple shuffles, e.g. page-rank (can use multiple MR but not very efficient)
  More flexible systems allow these, but more complex model.

总结
    MR大集群
        不是最高效和灵活的
        伸缩性很好
        编程也简单，容错比较好
    在实践中，你应该权衡一下，怎么使用
Conclusion
  MapReduce single-handedly made big cluster computation popular.
  - Not the most efficient or flexible.
  + Scales well.
  + Easy to program -- failures and data movement are hidden.
  These were good trade-offs in practice.
  We'll see some more advanced successors later in the course.
  Have fun with the lab!
